{"cells":[{"cell_type":"markdown","metadata":{"id":"d0W-WuYsjWPP"},"source":["# Artificial Intelligence Course - Fall 1402\n","## Computer Assignment #4 - Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"ksnYjMyNPAcn"},"source":["# Table of Contents\n","\n","- [Part 1: Value Iteration & Policy Iteration Algorithms](#1)\n","    - [َQuestion 1:](#1-0)\n","    - [َQuestion 2:](#1-1)\n","    - [َQuestion 3:](#1-12)\n","    - [َQuestion 4:](#1-2)\n","    - [َQuestion 5:](#1-3)\n","        - [Value Iteration](#1-3-1)\n","        - [Policy Iteration](#1-3-2)\n","    - [َQuestion 6:](#1-4)\n","        - [Value Iteration](#1-4-1)\n","        - [Policy Iteration](#1-4-2)\n","- [Part 2: Q-Learning Algorithm](#2)\n","    - [َQuestion 7:](#2-0)\n","    - [َQuestion 8:](#2-1)\n","    - [َQuestion 9:](#2-2)\n","    - [َQuestion 10:](#2-3)"]},{"cell_type":"code","source":["!pip install gym==0.25.2\n","!pip install numpy"],"metadata":{"id":"xyDFrJK3USwj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883024767,"user_tz":-210,"elapsed":9240,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"8e099da7-eb3e-46d6-d9fe-1371020a2ac8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dpTWKluXMHP5","executionInfo":{"status":"ok","timestamp":1706883031762,"user_tz":-210,"elapsed":466,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"outputs":[],"source":["# import\n","import numpy as np\n","import gym\n","import time\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"EifP8FUKLXE7"},"source":["<a name='1'></a>\n","## Part 1: Value Iteration & Policy Iteration Algorithms"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8LizJeOYRMEq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883036692,"user_tz":-210,"elapsed":11,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"daf2b33d-bb5c-44ef-936f-fe10ee937c04"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"YZARu6LDSqj_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883040733,"user_tz":-210,"elapsed":1465,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"1dc381e9-a6df-4c6f-8acd-61e15f5f8ab3"},"outputs":[{"output_type":"stream","name":"stdout","text":["you can see the environment in each step by render command :\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n","If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n"]}],"source":["# get familiar with the environment\n","print(\"you can see the environment in each step by render command :\")\n","env.reset()\n","env.render()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5QTTUSrSM-L","outputId":"a58dfea2-8541-478d-c8c1-df501af8a794","executionInfo":{"status":"ok","timestamp":1706883041317,"user_tz":-210,"elapsed":6,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":5}],"source":["# Total no. of states\n","env.observation_space.n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeaBqUeZSNNY","outputId":"ac78e8e9-2573-4676-ed6a-01429d7fcc93","executionInfo":{"status":"ok","timestamp":1706883045431,"user_tz":-210,"elapsed":681,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":6}],"source":["# Total no. of actions\n","env.action_space.n"]},{"cell_type":"markdown","source":["<a name='1-0'></a>\n","### Question 1:"],"metadata":{"id":"iVJmGmCUnIGR"}},{"cell_type":"markdown","source":["Value Iteration is a dynamic programming algorithm that we use to find the optimal policy and tackle reinforcement learning problems. It is an approach that wants to compute the optimal state values iteratively. Firstly, we initialize the values of all states arbitrarily and consider a small positive number for Θ>0 as our accuracy of estimation. Next, we update each state's value using the Bellman equation. Precisely, value iteration is obtained by turning the Bellman equation into an update rule. This value iteration update is identical to the policy evaluation update, except the fact that it requires the maximum over all actions. This equation is written below: [1]\n","\n","$$v_{k+1}(s)=max_{a}\\sum_{s^{'},r}p(s^{'},r|s,a)[r+\\gamma v_{k}(s^{'})]$$\n","\n","While we update iteratively, we check whether the maximum change in the values of states in the current iteration is below our threshold or not. After the convergence of our values, we can drive the optimal policy from the computed values, and for each state, we choose the action that maximizes our state value. Hence, we can write: [1]\n","\n","$$\\pi(s)=argmax_{a}\\sum_{s^{'},r}p(s^{'},r|s,a)[r+\\gamma v_{k}(s^{'})]$$\n","\n","Thus, we iteratively improved our estimates for state values using dynamic programming and value iteration algorithm."],"metadata":{"id":"xRDhQMwwnK2s"}},{"cell_type":"markdown","metadata":{"id":"MO24LtBGLXZ7"},"source":["<a name='1-1'></a>\n","### Question 2:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"VKP8LjK5jGoW","executionInfo":{"status":"ok","timestamp":1706883049373,"user_tz":-210,"elapsed":394,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"outputs":[],"source":["class ValueIteration():\n","    def __init__(self, env, discount_factor, theta=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.reset()\n","        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n","        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n","        self.state_values[self.env.observation_space.n - 1] = 0\n","        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n","\n","    def value_estimation(self):\n","        self.delta = np.inf\n","\n","        while(self.delta > self.theta):\n","\n","            self.delta = 0\n","\n","            for state in range(self.env.observation_space.n):\n","\n","                v = self.state_values[state]\n","\n","                for action in range(self.env.action_space.n):\n","                    action_value = 0\n","                    for probability, next_state, reward, done in self.env.P[state][action]:\n","                         action_value += probability*(reward+self.discount_factor*self.state_values[next_state])\n","                    self.q_values[state, action] = action_value\n","\n","                self.state_values[state] = np.max(self.q_values[state,:])\n","\n","                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n","\n","    def take_action(self, action):\n","        next_state, reward, done, _ = self.env.step(action)\n","        return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","        return np.argmax(self.q_values[state,:])\n","\n","    def get_state_values(self):\n","        return self.state_values\n","\n","    def get_q_values(self):\n","        return self.q_values\n","\n","    def reset(self):\n","        initial_state = self.env.reset()\n","        return initial_state"]},{"cell_type":"markdown","source":["<a name='1-12'></a>\n","### Question 3:"],"metadata":{"id":"frjc5mR4ncm1"}},{"cell_type":"markdown","source":["Policy Iteration is a dynamic programming algorithm used for finding the optimal policy in reinforcement learning models. It consists of two main steps, called policy evaluation and policy improvement. Firstly, we initialize an arbitrarily policy and state value. Next, we use the iterative policy evaluation by updating each state's values using the Bellman equation under the current policy. Its equation is this: [1]\n","\n","$$v(s)=\\sum_{s^{'},r}p(s^{'},r|s,π(s))[r+\\gamma v(s^{'})]$$\n","\n","We repeat this equation until the change in state values is lower than our small threshold. For each state, we update the policy to be greedy with respect to our current state values, and this current policy is optimal if it does not change. Its representation is as below:\n","\n","$$π(s)=argmax_{a}\\sum_{s^{'},r}p(s^{'},r|s,a)[r+\\gamma v(s^{'})]$$\n","\n","We check if the policy has not changed, and if it changes, we repeat these steps."],"metadata":{"id":"AMJJpf1tnddF"}},{"cell_type":"markdown","metadata":{"id":"V4DcH5yJLXqH"},"source":["<a name='1-2'></a>\n","### Question 4:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"XjSb1lX147hd","executionInfo":{"status":"ok","timestamp":1706883064634,"user_tz":-210,"elapsed":502,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"outputs":[],"source":["class PolicyIteration():\n","    def __init__(self, env, discount_factor, theta=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.reset()\n","        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n","        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n","        self.state_values[self.env.observation_space.n - 1] = 0\n","        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n","        self.policy = np.random.randint(self.env.action_space.n, size=self.env.observation_space.n) # initial policy\n","        self.policy_stable = False\n","\n","    def policy_evaluation(self):\n","        self.delta = np.inf\n","\n","        while(self.delta >= self.theta):\n","\n","            self.delta = 0\n","\n","            for state in range(self.env.observation_space.n):\n","\n","                v = self.state_values[state]\n","\n","                new_state_value = 0\n","                for probability, next_state, reward, done in self.env.P[state][self.policy[state]]:\n","                    new_state_value += probability*(reward + self.discount_factor*self.state_values[next_state])\n","                self.state_values[state] = new_state_value\n","\n","                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n","\n","    def policy_improvement(self):\n","        self.policy_stable = True\n","\n","        for state in range(self.env.observation_space.n):\n","            old_policy = self.policy[state]\n","\n","            for action in range(self.env.action_space.n):\n","\n","                action_value = 0\n","                for probability, next_state, reward, done in self.env.P[state][action]:\n","                    action_value += probability*(reward+self.discount_factor*self.state_values[next_state])\n","                self.q_values[state, action] = action_value\n","\n","            self.policy[state] = np.argmax(self.q_values[state,:])\n","\n","            if old_policy != self.policy[state]:\n","                self.policy_stable = False\n","\n","    def policy_estimation(self):\n","        self.policy_stable = False\n","\n","        while not self.policy_stable:\n","            self.policy_evaluation()\n","            self.policy_improvement()\n","\n","    def take_action(self, action):\n","        next_state, reward, done, _ = self.env.step(action)\n","        return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","        return self.policy[state]\n","\n","    def get_state_values(self):\n","        return self.state_values\n","\n","    def get_q_values(self):\n","        return self.q_values\n","\n","    def reset(self):\n","        initial_state = self.env.reset()\n","        return initial_state"]},{"cell_type":"markdown","metadata":{"id":"u4G-kVjmLYj4"},"source":["<a name='1-3'></a>\n","### Question 5:"]},{"cell_type":"markdown","metadata":{"id":"PB651-ZY4vjE"},"source":["<a name='1-3-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","source":["optimal_policy_valueiteration = []\n","value_iteration = ValueIteration(env, discount_factor=0.9)\n","for i in range(100):\n","    value_iteration.value_estimation()\n","state_values = value_iteration.get_state_values()\n","q_values = value_iteration.get_q_values()\n","for state in range(env.observation_space.n):\n","    optimal_policy_valueiteration.append(value_iteration.get_optimal_policy(state))"],"metadata":{"id":"C65pHgAkedzT","executionInfo":{"status":"ok","timestamp":1706883070233,"user_tz":-210,"elapsed":388,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["state_values_table_valueiteration = np.array(state_values)\n","print('Here is our state value table for value iteration:')\n","print(state_values_table_valueiteration)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQrPnTG4_pXs","executionInfo":{"status":"ok","timestamp":1706883071772,"user_tz":-210,"elapsed":5,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"392b157f-ad9a-4196-82a9-31061d0defb3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our state value table for value iteration:\n","[5.90490000e-01 6.56100000e-01 7.29000000e-01 6.56100000e-01\n"," 6.56100000e-01 2.60700746e-12 8.10000000e-01 2.60700746e-12\n"," 7.29000000e-01 8.10000000e-01 9.00000000e-01 2.60700746e-12\n"," 2.60700746e-12 9.00000000e-01 1.00000000e+00 0.00000000e+00]\n"]}]},{"cell_type":"code","source":["q_values_table_valueiteration = np.array(q_values)\n","print('Here is our state-action value table for value iteration:')\n","print(q_values_table_valueiteration)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_uv7sT-p_sTg","executionInfo":{"status":"ok","timestamp":1706883073168,"user_tz":-210,"elapsed":7,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"7ade7a11-c0a9-43d2-b1b3-e5d507bbde1e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our state-action value table for value iteration:\n","[[5.31441000e-01 5.90490000e-01 5.90490000e-01 5.31441000e-01]\n"," [5.31441000e-01 2.60700746e-12 6.56100000e-01 5.90490000e-01]\n"," [5.90490000e-01 7.29000000e-01 5.90490000e-01 6.56100000e-01]\n"," [6.56100000e-01 2.60700746e-12 5.90490000e-01 5.90490000e-01]\n"," [5.90490000e-01 6.56100000e-01 2.60700746e-12 5.31441000e-01]\n"," [2.60700746e-12 2.60700746e-12 2.60700746e-12 2.60700746e-12]\n"," [2.34630672e-12 8.10000000e-01 2.60700746e-12 6.56100000e-01]\n"," [2.60700746e-12 2.60700746e-12 2.60700746e-12 2.60700746e-12]\n"," [6.56100000e-01 2.60700746e-12 7.29000000e-01 5.90490000e-01]\n"," [6.56100000e-01 8.10000000e-01 8.10000000e-01 2.34630672e-12]\n"," [7.29000000e-01 9.00000000e-01 2.60700746e-12 7.29000000e-01]\n"," [2.60700746e-12 2.60700746e-12 2.60700746e-12 2.60700746e-12]\n"," [2.60700746e-12 2.60700746e-12 2.60700746e-12 2.60700746e-12]\n"," [2.34630672e-12 8.10000000e-01 9.00000000e-01 7.29000000e-01]\n"," [8.10000000e-01 9.00000000e-01 1.00000000e+00 8.10000000e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}]},{"cell_type":"code","source":["optimal_policy_table_valueiteration = np.array(optimal_policy_valueiteration).reshape((4, 4))\n","print('Here is our optimal policy table for value iteration:')\n","print(optimal_policy_table_valueiteration)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BxA-dSr_wPJ","executionInfo":{"status":"ok","timestamp":1706883075739,"user_tz":-210,"elapsed":7,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"3e900485-d909-47d2-cdfa-bbce72a6c9c4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our optimal policy table for value iteration:\n","[[1 2 1 0]\n"," [1 0 1 0]\n"," [2 1 1 0]\n"," [0 2 2 0]]\n"]}]},{"cell_type":"code","source":["print('Here is our optimal policy graphical render for agent by using value iteration:')\n","first_state = env.reset()\n","env.render()\n","for i in range(15):\n","  time.sleep(2)\n","  action = optimal_policy_valueiteration[first_state]\n","  first_state = env.step(action)[0]\n","  env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dYRD2uUEPQ3","executionInfo":{"status":"ok","timestamp":1706883107325,"user_tz":-210,"elapsed":30230,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"5198d6f2-931f-440f-beff-0235bc647925"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our optimal policy graphical render for agent by using value iteration:\n"]}]},{"cell_type":"markdown","metadata":{"id":"ipZlzoXH40Mn"},"source":["<a name='1-3-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","source":["optimal_policy_policyiteration = []\n","policy_iteration = PolicyIteration(env, discount_factor=0.9)\n","for i in range(100):\n","    policy_iteration.policy_estimation()\n","state_values = policy_iteration.get_state_values()\n","q_values = policy_iteration.get_q_values()\n","for state in range(env.observation_space.n):\n","    optimal_policy_policyiteration.append(policy_iteration.get_optimal_policy(state))"],"metadata":{"id":"Qb9ijQh3fHqd","executionInfo":{"status":"ok","timestamp":1706883107326,"user_tz":-210,"elapsed":25,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["state_values_table_policyiteration = np.array(state_values)\n","print('Here is our state value table for policy iteration:')\n","print(state_values_table_policyiteration)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoyerNzw_1aD","executionInfo":{"status":"ok","timestamp":1706883107327,"user_tz":-210,"elapsed":24,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"7b5bfb6f-e1d0-4f64-b4c8-63e14877ca76"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our state value table for policy iteration:\n","[5.90490000e-01 6.56100000e-01 7.29000000e-01 6.56100000e-01\n"," 6.56100000e-01 5.96399686e-13 8.10000000e-01 5.96399686e-13\n"," 7.29000000e-01 8.10000000e-01 9.00000000e-01 5.96399686e-13\n"," 5.96399686e-13 9.00000000e-01 1.00000000e+00 0.00000000e+00]\n"]}]},{"cell_type":"code","source":["q_values_table_policyiteration = np.array(q_values)\n","print('Here is our state-action value table for policy iteration:')\n","print(q_values_table_policyiteration)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-zdozn6_3OG","executionInfo":{"status":"ok","timestamp":1706883107329,"user_tz":-210,"elapsed":22,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"c461f4aa-0276-413a-d811-5cb1b86d9c87"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our state-action value table for policy iteration:\n","[[5.31441000e-01 5.90490000e-01 5.90490000e-01 5.31441000e-01]\n"," [5.31441000e-01 5.36759718e-13 6.56100000e-01 5.90490000e-01]\n"," [5.90490000e-01 7.29000000e-01 5.90490000e-01 6.56100000e-01]\n"," [6.56100000e-01 5.36759718e-13 5.90490000e-01 5.90490000e-01]\n"," [5.90490000e-01 6.56100000e-01 5.36759718e-13 5.31441000e-01]\n"," [5.36759718e-13 5.36759718e-13 5.36759718e-13 5.36759718e-13]\n"," [5.36759718e-13 8.10000000e-01 5.36759718e-13 6.56100000e-01]\n"," [5.36759718e-13 5.36759718e-13 5.36759718e-13 5.36759718e-13]\n"," [6.56100000e-01 5.36759718e-13 7.29000000e-01 5.90490000e-01]\n"," [6.56100000e-01 8.10000000e-01 8.10000000e-01 5.36759718e-13]\n"," [7.29000000e-01 9.00000000e-01 5.36759718e-13 7.29000000e-01]\n"," [5.36759718e-13 5.36759718e-13 5.36759718e-13 5.36759718e-13]\n"," [5.36759718e-13 5.36759718e-13 5.36759718e-13 5.36759718e-13]\n"," [5.36759718e-13 8.10000000e-01 9.00000000e-01 7.29000000e-01]\n"," [8.10000000e-01 9.00000000e-01 1.00000000e+00 8.10000000e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}]},{"cell_type":"code","source":["optimal_policy_table_policyiteration = np.array(optimal_policy_policyiteration).reshape((4, 4))\n","print('Here is our optimal policy table for policy iteration:')\n","print(optimal_policy_table_policyiteration)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DcFQ0R3_8Da","executionInfo":{"status":"ok","timestamp":1706883107329,"user_tz":-210,"elapsed":16,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"7327dfdf-0a73-4190-acc7-109dd1d2e385"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our optimal policy table for policy iteration:\n","[[1 2 1 0]\n"," [1 0 1 0]\n"," [2 1 1 0]\n"," [0 2 2 0]]\n"]}]},{"cell_type":"code","source":["print('Here is our optimal policy graphical render for agent by using policy iteration:')\n","first_state = env.reset()\n","env.render()\n","for i in range(15):\n","  time.sleep(2)\n","  action = optimal_policy_policyiteration[first_state]\n","  first_state = env.step(action)[0]\n","  env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TC12RuecK9YV","executionInfo":{"status":"ok","timestamp":1706883137675,"user_tz":-210,"elapsed":30359,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"af3aa6a4-edf1-4d1b-9a90-243059878b67"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our optimal policy graphical render for agent by using policy iteration:\n"]}]},{"cell_type":"markdown","metadata":{"id":"A3fnAFqJLpVI"},"source":["<a name='1-4'></a>\n","### Question 6:"]},{"cell_type":"markdown","source":["Both the value and policy iteration methods returned the exact same policy. In terms of state values, there are slight differences in some states. Value iteration method returned slightly higher values for some of the states compared to policy iteration. However, this difference did not change our policy. By using the below code cells, we can see that the policy iteration method is much faster than value iteration.\n"],"metadata":{"id":"6RaL_OHr7h4O"}},{"cell_type":"markdown","metadata":{"id":"7JuoMPH_PAcv"},"source":["<a name='1-4-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"vCBnAicAPAcv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883142350,"user_tz":-210,"elapsed":4685,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"d63acead-8a5b-4d68-cee8-1e89711551cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Value iteration elapsed time for hundred episodes is: 4.895099124000012\n"]}],"source":["time_elapsed_value_iteration = 0\n","for i in range(100):\n","    value_iteration = ValueIteration(env, discount_factor=0.9)\n","    time_1 = time.perf_counter() #Return the value of a performance counter\n","    value_iteration.value_estimation()\n","    time_2 = time.perf_counter()\n","    time_elapsed_value_iteration = time_elapsed_value_iteration + time_2 - time_1\n","print('Value iteration elapsed time for hundred episodes is: '+str(time_elapsed_value_iteration))"]},{"cell_type":"markdown","metadata":{"id":"C1BMk-BfPAcv"},"source":["<a name='1-4-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"LnUZpvLP446n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883145029,"user_tz":-210,"elapsed":2695,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"1f792e1b-0619-480b-eacd-e2d8af4c8dc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Value iteration elapsed time for hundred episodes is: 2.19284545399978\n"]}],"source":["time_elapsed_policy_iteration = 0\n","for i in range(100):\n","    policy_iteration = PolicyIteration(env, discount_factor=0.9)\n","    time_1 = time.perf_counter() #Return the value of a performance counter\n","    policy_iteration.policy_estimation()\n","    time_2 = time.perf_counter()\n","    time_elapsed_policy_iteration = time_elapsed_policy_iteration + time_2 - time_1\n","print('Value iteration elapsed time for hundred episodes is: '+str(time_elapsed_policy_iteration))"]},{"cell_type":"markdown","metadata":{"id":"hLtPLm-ELpG9"},"source":["<a name='2'></a>\n","## Part 2: Q-Learning Algorithm"]},{"cell_type":"markdown","source":["<a name='2-0'></a>\n","### Question 7:"],"metadata":{"id":"S7yRce6rSlhY"}},{"cell_type":"markdown","source":["Q-learning is an off-policy algorithm, and this means that this algorithm learns from the experiences of a policy that is different from our target policy. This policy can be driven by an exploration strategy, in which it explores the environment to discover our optimal actions. In this algorithm, we want to learn the state-action values, and they are calculated from this equation using the Bellman equation to update Q-values: [1]\n","\n","$$Q(S_{t},A_{t})= Q(S_{t},A_{t}) + α[R_{t+1} + γmax_aQ(S_{t+1},a) - Q(S_{t},A_{t})]$$\n","\n","Q-learning uses exploration and exploitation. With probability ϵ our algorithm selects a random action, and with probability 1-ϵ it selects the action with the highest Q-value. Hence, by using this algorithm, we can find the optimal action-value function independent of following a policy."],"metadata":{"id":"qlpNdHs8SwAk"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"3cW_rkeDMOE8","executionInfo":{"status":"ok","timestamp":1706883145031,"user_tz":-210,"elapsed":18,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"outputs":[],"source":["# hyperparameters\n","REPS = 20\n","EPISODES = 2000\n","EPSILON = 0.1\n","LEARNING_RATE = 0.1\n","DISCOUNT = 0.9\n","STUDENT_NUM = 280"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"L1c1w7tRMOR_","outputId":"6e027108-2c40-48b1-9d94-8050c558105a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883149748,"user_tz":-210,"elapsed":6,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]},{"output_type":"execute_result","data":{"text/plain":["494"]},"metadata":{},"execution_count":22}],"source":["# environment\n","env = gym.make('Taxi-v3')\n","env.seed(seed = 280)\n","Initial_State = env.reset()\n","Initial_State"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"ou0fWiX_MsZb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883151261,"user_tz":-210,"elapsed":4,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"5f16a551-96d8-470e-bdbd-7fde08af8f21"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 4, 3, 2)"]},"metadata":{},"execution_count":23}],"source":["taxi_row, taxi_col, pass_idx, dest_idx = env.decode(Initial_State)\n","taxi_row, taxi_col, pass_idx, dest_idx"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"-8tJoWefMOdT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883152300,"user_tz":-210,"elapsed":632,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"64ed3baf-f931-4a85-f471-76dfd338b4db"},"outputs":[{"output_type":"stream","name":"stdout","text":["you can see the environment in each step by render command :\n"]}],"source":["# get familiar with the environment\n","print(\"you can see the environment in each step by render command :\")\n","env.render()"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"mQCB-ZIfNwJM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883152811,"user_tz":-210,"elapsed":14,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"b3c16973-d4ad-4cbf-d8a2-6f5f58778beb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["500"]},"metadata":{},"execution_count":25}],"source":["# Total no. of states\n","env.observation_space.n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"lmk_EYbKNwYT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883154014,"user_tz":-210,"elapsed":7,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"0a80970c-ad2d-4c95-8af2-c16ffd5cb1ce"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":26}],"source":["# Total no. of actions\n","env.action_space.n"]},{"cell_type":"markdown","metadata":{"id":"oZJAP4nMLpiZ"},"source":["<a name='2-1'></a>\n","### Question 8:"]},{"cell_type":"markdown","source":["In our first episode, the value of Epsilon is one, and in further episodes, our value of Epsilon will decreases exponentially.\n","\n","We used this method for our learning rate, except for the fact that our learning rate at episode zero is 0.99."],"metadata":{"id":"M1qv0sAkBsNl"}},{"cell_type":"code","execution_count":27,"metadata":{"id":"Ue4m5fg9450B","executionInfo":{"status":"ok","timestamp":1706883156122,"user_tz":-210,"elapsed":7,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"outputs":[],"source":["class QLearningAgent():\n","    def __init__(self, env, epsilon, learning_rate, discount_factor, seed):\n","      self.env = env\n","      self.epsilon = epsilon\n","      self.learning_rate = learning_rate\n","      self.olr = learning_rate\n","      self.discount_factor = discount_factor\n","      self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n","      self.seed = seed\n","\n","    def choose_action(self, state):\n","      if np.random.rand() < self.epsilon:\n","        action = np.random.choice(self.env.action_space.n)\n","      else:\n","        action = self.get_optimal_policy(state)\n","      return action\n","\n","    def update_q_table(self, state, action, nextState, reward):\n","      self.q_table[state][action] = self.q_table[state][action] + self.learning_rate*(reward+self.discount_factor*np.max(self.q_table[int(nextState),:])-(self.q_table[state][action]))\n","\n","    def decay_epsilon(self, episode):\n","       self.epsilon = np.exp(-0.002*(episode+1))\n","\n","    def decrease_learning_rate(self, episode):\n","      self.learning_rate = 0.99*np.exp(-0.002*(episode))\n","\n","    def take_action(self, action):\n","      next_state, reward, done, _ = self.env.step(action)\n","      return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","      return np.argmax(self.q_table[state])\n","\n","    def get_q_values(self):\n","      return self.q_table\n","\n","    def reset(self):\n","      return self.env.reset(seed=self.seed)"]},{"cell_type":"markdown","metadata":{"id":"c5HFAMk-Lpvs"},"source":["<a name='2-2'></a>\n","### Question 9:"]},{"cell_type":"code","source":["agent = QLearningAgent(env, epsilon=0.1, learning_rate=0.1, discount_factor=0.9, seed=280)\n","\n","optimal_state_values = []\n","optimal_q_values = []\n","optimal_policies = []\n","\n","for repeat in range(REPS):\n","    env.seed(seed = 280)\n","    agent.reset()\n","\n","    for episode in range(EPISODES):\n","        env.seed(seed = 280)\n","        state = agent.reset()\n","        total_reward = 0\n","\n","        while True:\n","            action = agent.choose_action(state)\n","            next_state, reward, done = agent.take_action(action)\n","            agent.update_q_table(state, action, next_state, reward)\n","            total_reward += reward\n","            state = next_state\n","\n","            if done:\n","                break\n","\n","        agent.decay_epsilon(episode)\n","        agent.decrease_learning_rate(episode)\n","\n","    optimal_state_values.append(np.max(agent.q_table, axis=1))\n","    optimal_q_values.append(agent.get_q_values())\n","for state in range(env.observation_space.n):\n","    optimal_policies.append(np.argmax(optimal_q_values[19][state]))"],"metadata":{"id":"EVq5zY8GPeJn","executionInfo":{"status":"ok","timestamp":1706883203476,"user_tz":-210,"elapsed":44791,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["print('Here is our state values:')\n","print(optimal_state_values[19])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"otv7IJ8Uvm1a","executionInfo":{"status":"ok","timestamp":1706883361685,"user_tz":-210,"elapsed":387,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"53a2f2f6-accf-49ad-c975-c484545810de"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our state values:\n","[ 0.          0.          7.7147      0.          0.          0.\n"," -4.99684549  0.          0.          0.          0.          0.\n","  0.          0.         -3.82326604  0.          0.          0.\n","  9.683       0.          0.          0.          5.94323     0.\n","  0.          0.         -4.44093943  0.          0.          0.\n","  0.          0.          0.          0.         -3.13696226  0.\n","  0.          0.          7.7147      0.          0.          0.\n"," -0.58568212  0.          0.          0.         -0.58568212  0.\n","  0.          0.          0.          0.          0.          0.\n"," -2.37440252  0.          0.          0.          5.94323     0.\n","  0.          0.         -1.52711391  0.          0.          0.\n","  0.4603532   0.          0.          0.          0.          0.\n","  0.          0.         -1.52711391  0.          0.          0.\n","  4.348907    0.          0.          0.         -2.3744027   0.\n","  0.          0.          1.62261467  0.          0.          0.\n","  0.          0.          0.          0.         -2.37440252  0.\n","  0.          0.          2.9140163   0.          0.          0.\n","  5.94323     0.          0.          0.         -4.44093943  0.\n","  0.          0.          0.          0.          0.          0.\n"," -3.13696226  0.          0.          0.         11.87        0.\n","  0.          0.          4.348907    0.          0.          0.\n"," -3.82326604  0.          0.          0.          0.          0.\n","  0.          0.         -2.37440252  0.          0.          0.\n","  9.683       0.          0.          0.          0.4603532   0.\n","  0.          0.         -1.52711391  0.          0.          0.\n","  0.          0.          0.          0.         -1.52711391  0.\n","  0.          0.          7.7147      0.          0.          0.\n"," -0.58568212  0.          0.          0.         -0.58568212  0.\n","  0.          0.          0.          0.          0.          0.\n"," -0.58568212  0.          0.          0.          5.94323     0.\n","  0.          0.         -1.52711391  0.          0.          0.\n","  0.4603532   0.          0.          0.          0.          0.\n","  0.          0.         -1.52711391  0.          0.          0.\n","  4.348907    0.          0.          0.          4.348907    0.\n","  0.          0.         -3.82326604  0.          0.          0.\n","  0.          0.          0.          0.         -2.37440252  0.\n","  0.          0.         14.3         0.          0.          0.\n","  2.9140163   0.          0.          0.         -3.13696226  0.\n","  0.          0.          0.          0.          0.          0.\n"," -1.52711391  0.          0.          0.         11.87        0.\n","  0.          0.          1.62261467  0.          0.          0.\n"," -2.37440252  0.          0.          0.          0.          0.\n","  0.          0.         -0.58568212  0.          0.          0.\n","  9.683       0.          0.          0.          0.4603532   0.\n","  0.          0.         -1.52711391  0.          0.          0.\n","  0.          0.          0.          0.          0.4603532   0.\n","  0.          0.          7.7147      0.          0.          0.\n"," -0.58568217  0.          0.          0.         -0.58568212  0.\n","  0.          0.          0.          0.          0.          0.\n"," -0.58568212  0.          0.          0.          5.94323     0.\n","  0.          0.          2.9140163   0.          0.          0.\n"," -4.44093943  0.          0.          0.          0.          0.\n","  0.          0.         -3.13696226  0.          0.          0.\n"," 17.          0.          0.          0.          1.62261467  0.\n","  0.          0.         -3.82326604  0.          0.          0.\n","  0.          0.          0.          0.         -2.37440252  0.\n","  0.          0.          9.683       0.          0.          0.\n","  0.4603532   0.          0.          0.         -3.13696226  0.\n","  0.          0.          0.          0.          0.          0.\n"," -1.52711391  0.          0.          0.          7.7147      0.\n","  0.          0.         -0.58568212  0.          0.          0.\n"," -2.37440252  0.          0.          0.          0.          0.\n","  0.          0.          1.62261467  0.          0.          0.\n","  5.94323     0.          0.          0.         -1.52711394  0.\n","  0.          0.         -1.52711391  0.          0.          0.\n","  0.          0.          0.          0.          0.4603532   0.\n","  0.          0.          4.348907    0.          0.          0.\n","  1.62261467  0.          0.          0.         -4.99684549  0.\n","  0.          0.          0.          0.          0.          0.\n"," -3.82326604  0.          0.          0.         20.          0.\n","  0.          0.          0.4603532   0.          0.          0.\n"," -4.44093943  0.          0.          0.          0.          0.\n","  0.          0.         -3.13696226  0.          0.          0.\n","  7.7147      0.          0.          0.         -0.58568212  0.\n","  0.          0.         -3.82326604  0.          0.          0.\n","  0.          0.          0.          0.         -2.37440252  0.\n","  0.          0.          5.94323     0.          0.          0.\n"," -1.52711391  0.          0.          0.         -3.13696226  0.\n","  0.          0.          0.          0.          0.          0.\n","  2.9140163   0.          0.          0.          4.348907    0.\n","  0.          0.         -2.37440252  0.          0.          0.\n"," -2.37440252  0.          0.          0.          0.          0.\n","  0.          0.          1.62261467  0.          0.          0.\n","  2.9140163   0.        ]\n"]}]},{"cell_type":"code","source":["print('Here is our state-action values:')\n","print(optimal_q_values[19])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OL43-z8PvoDa","executionInfo":{"status":"ok","timestamp":1706883364877,"user_tz":-210,"elapsed":9,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"48afd858-0089-44cb-d747-8d08d763c190"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our state-action values:\n","[[ 0.          0.          0.          0.          0.          0.        ]\n"," [ 0.          0.          0.          0.          0.          0.        ]\n"," [ 4.348907    5.94323     4.348907    5.94323     7.7147     -3.05677   ]\n"," ...\n"," [ 0.          0.          0.          0.          0.          0.        ]\n"," [ 1.62261467  2.9140163   1.62261467  2.9140163  -7.37738533 -7.37738533]\n"," [ 0.          0.          0.          0.          0.          0.        ]]\n"]}]},{"cell_type":"code","source":["print('Here is our optimal policy:')\n","print(optimal_policies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mpliJxxvpR9","executionInfo":{"status":"ok","timestamp":1706883365990,"user_tz":-210,"elapsed":490,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"2b860973-2d59-4b25-9f82-9bb56b47184c"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our optimal policy:\n","[0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"BVcMKEGDQWdU"},"source":["<a name='2-3'></a>\n","### Question 10:"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"FlHPV0kqQWdU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706883414492,"user_tz":-210,"elapsed":45538,"user":{"displayName":"Ali Ghanizadeh","userId":"09568708236137211125"}},"outputId":"8f427595-e835-4833-f8e3-66f018204349"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here is our optimal policy graphical render for our agent:\n"]}],"source":["print('Here is our optimal policy graphical render for our agent:')\n","env.seed(seed = 280)\n","first_state = env.reset()\n","env.render()\n","for i in range(15):\n","  action = optimal_policies[first_state]\n","  first_state = env.step(action)[0]\n","  time.sleep(3)\n","  env.render()"]},{"cell_type":"markdown","source":["# Reference\n","\n"],"metadata":{"id":"Qdw1DIWZwU-D"}},{"cell_type":"markdown","source":["1. Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning, second edition: An Introduction. MIT Press. ISBN 978-0-262-03924-6."],"metadata":{"id":"L_trjqApwcCv"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}